{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "standard-mercy",
   "metadata": {
    "id": "standard-mercy"
   },
   "source": [
    "# Detectron2 Object Detection\n",
    "### Overview\n",
    "* Train an instance segmentation model using detecton2\n",
    "* Convert Labelbox data into the mscoco object detection format\n",
    "* Upload predictions to the Labelbox model diagnostics tool\n",
    "* Upload predictions for MAL to accelerate labeling efforts\n",
    "\n",
    "### Usage\n",
    "- <b>Model Training</b>:\n",
    "  * Set a project ID containing polygons and/or rectangles\n",
    "- <b>Diagnostics</b>:\n",
    "  * No additional configuration is necessary. As long as the model has been   \n",
    "- <b>MAL</b>:\n",
    "  * Set a dataset ID for the dataset you would like to upload predictions to. A new project will automatically be created.\n",
    "trained this will work.\n",
    "\n",
    "### Suggested Workflow\n",
    "* To get the most out of Labelbox, we suggest training a model on a small amount of data, exploring model performance using diagnostics, selecting a dataset using catalog to address model shortcomings, make any model architecture adjustments, and then upload predictions via MAL made on this new dataset for faster labeling.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UjiT96PYGE8c",
   "metadata": {
    "id": "UjiT96PYGE8c"
   },
   "source": [
    "<b>Setup</b>:\n",
    "* This is the only section that needs to be configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whhO0QuXGDru",
   "metadata": {
    "id": "whhO0QuXGDru"
   },
   "outputs": [],
   "source": [
    "API_KEY = None\n",
    "# For training:\n",
    "project_id = \"\"\n",
    "# The model will make predictions on the following dataset \n",
    "# and upload predictions to a new project for model assisted labeling.\n",
    "mal_dataset_id = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ydhOSHXCgFde",
   "metadata": {
    "id": "ydhOSHXCgFde"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-hindu",
   "metadata": {
    "id": "young-hindu"
   },
   "outputs": [],
   "source": [
    "# Based on:\n",
    "# https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5#scrollTo=Ya5nEuMELeq8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "broadband-commission",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "broadband-commission",
    "outputId": "054f39a3-c353-467c-cbdf-4ace18528949"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'git+https://github.com/facebookresearch/detectron2.git'\"\n",
      "WARNING: You are using pip version 20.1.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\fakec\\onedrive - the university of auckland\\2022\\700\\machines-at-work-satellite-imagery\\env\\scripts\\python.exe -m pip install --upgrade pip' command.\n",
      "  WARNING: Did not find branch or tag 'ms/coco', assuming revision or ref.\n",
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: git checkout -q ms/coco\n",
      "       cwd: C:\\Users\\fakec\\AppData\\Local\\Temp\\pip-install-um8zw5zz\\labelbox\n",
      "  Complete output (1 lines):\n",
      "  error: pathspec 'ms/coco' did not match any file(s) known to git\n",
      "  ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: git checkout -q ms/coco Check the logs for full command output.\n",
      "WARNING: You are using pip version 20.1.1; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\fakec\\onedrive - the university of auckland\\2022\\700\\machines-at-work-satellite-imagery\\env\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch \\\n",
    "                torchvision \\\n",
    "                git+https://github.com/cocodataset/panopticapi.git \\\n",
    "                'git+https://github.com/facebookresearch/detectron2.git'           \n",
    "!pip install -q \"git+https://github.com/Labelbox/labelbox-python@ms/coco#egg=labelbox[data]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-feeding",
   "metadata": {
    "id": "systematic-feeding"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm import tqdm \n",
    "from shapely.geometry import MultiPolygon\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.data import build_detection_test_loader\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data.datasets.coco import load_coco_json\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "from labelbox import Client, LabelingFrontend, OntologyBuilder\n",
    "from labelbox.data.serialization import COCOConverter, NDJsonConverter\n",
    "from labelbox.schema.model import Model\n",
    "from labelbox.data.metrics.group import get_label_pairs\n",
    "from labelbox.data.annotation_types import (\n",
    "    Mask, \n",
    "    MaskData, \n",
    "    ObjectAnnotation, \n",
    "    LabelList, \n",
    "    Point, \n",
    "    Rectangle, \n",
    "    Polygon, \n",
    "    ImageData, \n",
    "    Label,\n",
    "    ScalarMetric\n",
    ")\n",
    "from labelbox.data.metrics import (\n",
    "    feature_miou_metric, \n",
    "    feature_confusion_matrix_metric\n",
    ")\n",
    "\n",
    "with open('./coco_utils.py', 'w' ) as file:\n",
    "    helper = requests.get(\"https://raw.githubusercontent.com/Labelbox/labelbox-python/ms/coco/examples/integrations/detectron2/coco_utils.py\").text\n",
    "    file.write(helper)\n",
    "from coco_utils import visualize_coco_examples, visualize_object_inferences, partition_coco    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vq-QYq6r70Ug",
   "metadata": {
    "id": "vq-QYq6r70Ug"
   },
   "outputs": [],
   "source": [
    "client = Client(api_key = API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-windows",
   "metadata": {
    "id": "binding-windows"
   },
   "source": [
    "## Optional Config:\n",
    "* `project_id` - Indicates which project labels should be exported from.\n",
    "* `mal_dataset_id` - Dataset to use for MAL\n",
    "* `image_root` - Where to write images to on disk\n",
    "* `mask_root` - Where to write semantic segmentation masks to on disk\n",
    "* `train_json_path` - Where the test partition coco dataset will be written to\n",
    "* `train_json_path` - Where the train partition coco dataset will be written to\n",
    "* `train_test_split` - How much of the data to add to each parition (by percent)\n",
    "* `model_zoo_config` - Detectron2 model config see more here : https://github.com/facebookresearch/detectron2/blob/master/detectron2/model_zoo/model_zoo.py \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-distribution",
   "metadata": {
    "id": "facial-distribution"
   },
   "outputs": [],
   "source": [
    "# These don't need to be set\n",
    "image_root = \"/tmp/images/\"\n",
    "mask_root = \"/tmp/masks/\"\n",
    "train_json_path = '/tmp/json_train_annotations.json'\n",
    "test_json_path = '/tmp/json_test_annotations.json'\n",
    "train_test_split = [0.8, 0.2]\n",
    "model_zoo_config = \"COCO-InstanceSegmentation/mask_rcnn_R_50_C4_1x.yaml\"\n",
    "\n",
    "# These can be set to anything. As long as this process doesn't have \n",
    "# another dataset with these names\n",
    "train_ds_name = \"custom_coco_train\"\n",
    "test_ds_name = \"custom_coco_test\"\n",
    "\n",
    "model_name = \"detectron_object_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-shape",
   "metadata": {
    "id": "gothic-shape"
   },
   "source": [
    "* Set up directory structure\n",
    "* Create Train/Test/Val Partition\n",
    "    - Test is for testing locally\n",
    "    - Val is is for tracking over time in labelbox\n",
    "\n",
    "\n",
    "* Add your own data. It will attempt to turn masks into instances.\n",
    "* If you want panoptic segmentation follow the panoptic tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-handy",
   "metadata": {
    "id": "decreased-handy"
   },
   "outputs": [],
   "source": [
    "proj = client.get_project(project_id)\n",
    "for path in [image_root, mask_root]:\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JUPHWPbDrCZs",
   "metadata": {
    "id": "JUPHWPbDrCZs"
   },
   "source": [
    "* Create Train and Test data for local dev\n",
    "* Use val data for Diagnostics and to track progress over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K08amRitrBz_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K08amRitrBz_",
    "outputId": "104a7204-8703-4369-8669-86b51b7799e8"
   },
   "outputs": [],
   "source": [
    "labels = proj.label_generator()\n",
    "val_labels = [next(labels) for idx in range(25)]\n",
    "\n",
    "\n",
    "coco = COCOConverter.serialize_instances(\n",
    "    labels = labels, \n",
    "    image_root = image_root\n",
    ")\n",
    "train_partition, test_partition = partition_coco(coco, splits = [0.8, 0.2])\n",
    "\n",
    "for parition, file_name in [[train_partition, train_json_path], [test_partition, test_json_path]]:\n",
    "    with open(file_name, 'w') as file:\n",
    "        json.dump(parition['instance'], file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IK6yVXswgLHr",
   "metadata": {
    "id": "IK6yVXswgLHr"
   },
   "source": [
    "## Explore Data\n",
    "* Register detectron datasets\n",
    "* Visualize the data to make sure everything was converted properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-melissa",
   "metadata": {
    "id": "unexpected-melissa"
   },
   "outputs": [],
   "source": [
    "register_coco_instances(train_ds_name, {} , train_json_path, image_root)\n",
    "register_coco_instances(test_ds_name , {} , test_json_path, image_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-estonia",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "boolean-estonia",
    "outputId": "f117a60e-2974-49f1-f876-21dbc4f96c55"
   },
   "outputs": [],
   "source": [
    "\n",
    "MetadataCatalog.get(test_ds_name).thing_classes = {r['id'] : r['name'] for r in coco['categories']}\n",
    "test_json = load_coco_json(test_json_path, image_root)\n",
    "visualize_coco_examples(MetadataCatalog.get(test_ds_name), test_json , resize_dims = (768, 512), max_images = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JuX_rnqPgUTf",
   "metadata": {
    "id": "JuX_rnqPgUTf"
   },
   "source": [
    "## Train Model\n",
    "* Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-ebony",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "above-ebony",
    "outputId": "720f8b23-c740-405d-d7e6-5691c22f2e45"
   },
   "outputs": [],
   "source": [
    "# Clear metadata so detectron recomputes.\n",
    "if hasattr(MetadataCatalog.get(train_ds_name), 'thing_classes'):\n",
    "    del MetadataCatalog.get(train_ds_name).thing_classes\n",
    "if hasattr(MetadataCatalog.get(test_ds_name), 'thing_classes'):\n",
    "    del MetadataCatalog.get(test_ds_name).thing_classes    \n",
    "\n",
    "# Set model config.\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(model_zoo_config))\n",
    "cfg.DATASETS.TRAIN = (train_ds_name,)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_zoo_config)  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "cfg.SOLVER.MAX_ITER = 120\n",
    "cfg.SOLVER.STEPS = []        \n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(coco['categories']) \n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FwXZGrWBgcEj",
   "metadata": {
    "id": "FwXZGrWBgcEj"
   },
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-alias",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "assumed-alias",
    "outputId": "a8d8ab7a-7a1d-4634-c6fa-c0e7c22729f8"
   },
   "outputs": [],
   "source": [
    "# Use this for validation if you would like..\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)\n",
    "evaluator = COCOEvaluator(test_ds_name)\n",
    "val_loader = build_detection_test_loader(cfg, test_ds_name)\n",
    "print(inference_on_dataset(trainer.model, val_loader, evaluator))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RAysakmWge-G",
   "metadata": {
    "id": "RAysakmWge-G"
   },
   "source": [
    "## Visualize Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-mandate",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "casual-mandate",
    "outputId": "1b7c6296-b282-494e-e721-b8b4a658e545"
   },
   "outputs": [],
   "source": [
    "# We can use `Visualizer` to draw the predictions on the image.\n",
    "\n",
    "# Export Data From Catalog or another dataset here..\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7 \n",
    "predictor = DefaultPredictor(cfg)\n",
    "test_json = load_coco_json(test_json_path, image_root)\n",
    "del MetadataCatalog.get(test_ds_name).thing_classes\n",
    "MetadataCatalog.get(test_ds_name).thing_classes = {idx : r['name'] for idx, r in enumerate(coco['categories'])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "riOH9U7bsBKV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "riOH9U7bsBKV",
    "outputId": "ef0f6186-acc9-4830-a96d-b885a1313a83"
   },
   "outputs": [],
   "source": [
    " visualize_object_inferences(MetadataCatalog.get(test_ds_name),test_json, predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-return",
   "metadata": {
    "id": "focused-return"
   },
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "predictor = DefaultPredictor(cfg)\n",
    "# for single test image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zE2KjGglgpuT",
   "metadata": {
    "id": "zE2KjGglgpuT"
   },
   "source": [
    "## Make Predictions\n",
    "* Create helper function for making inferences\n",
    "* Predict and upload for model assisted labeling\n",
    "* Predict and upload for model diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2QYGyr0RXPKv",
   "metadata": {
    "id": "2QYGyr0RXPKv"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_label(image):\n",
    "  # This is a bit slow, there is some i/o for downloading the images but then inference is slow\n",
    "  # Runs inference on an image and returns a label\n",
    "    res = predictor(image.value)\n",
    "    annotations = []\n",
    "    for idx in range(len(res['instances'])):\n",
    "        mask = MaskData.from_2D_arr(res['instances'][idx].pred_masks[0].cpu().numpy().astype(np.uint8))\n",
    "        mask = Mask(mask = mask, color = (1,1,1))\n",
    "        geom = mask.shapely.buffer(0).simplify(3)\n",
    "        if isinstance(geom, MultiPolygon):\n",
    "            geom = geom.convex_hull\n",
    "\n",
    "        annotations.append(ObjectAnnotation(\n",
    "            name = MetadataCatalog.get(test_ds_name).thing_classes[res['instances'][idx].pred_classes[0].cpu().numpy().item()],\n",
    "            value = Polygon(points = [Point(x=x,y=y) for x,y in list(geom.exterior.coords)]),\n",
    "        ))\n",
    "    return Label(data = image, annotations = annotations)\n",
    "\n",
    "# Allows us to upload local images to labelbox\n",
    "signer = lambda _bytes: client.upload_data(content=_bytes, sign=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VvfTBjYxw07Q",
   "metadata": {
    "id": "VvfTBjYxw07Q"
   },
   "source": [
    "## Upload for Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gWnQ16A_w1FV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gWnQ16A_w1FV",
    "outputId": "e3a3903c-f93c-4515-a373-acf31676e3aa"
   },
   "outputs": [],
   "source": [
    "labels_mea = LabelList()\n",
    "\n",
    "with ThreadPoolExecutor(4) as executor:\n",
    "    futures = [executor.submit(get_label, label.data) for label in val_labels]\n",
    "    for future in tqdm(as_completed(futures)):\n",
    "        labels_mea.append(future.result())\n",
    "\n",
    "labels_mea.add_url_to_masks(signer) \\\n",
    "      .add_url_to_data(signer) \\\n",
    "      .assign_feature_schema_ids(OntologyBuilder.from_project(proj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cINUr6fnOrE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cINUr6fnOrE",
    "outputId": "9b35aed7-549d-4945-f31a-b8c51a386ee7"
   },
   "outputs": [],
   "source": [
    "# If the model already exists fetch it with the following:\n",
    "\n",
    "model = next(client.get_models(where = Model.name == model_name), None)\n",
    "if model is None:\n",
    "    model = client.create_model(model_name, ontology_id=proj.ontology().uid)\n",
    "\n",
    "\n",
    "# Increment model run version if it exists. Otherwise use the initial 0.0.0\n",
    "model_run_names = [model_run.name for model_run in model.model_runs()]\n",
    "if len(model_run_names):\n",
    "    model_run_names.sort(key=lambda s: [int(u) for u in s.split('.')])\n",
    "    latest_model_run_name = model_run_names[-1]\n",
    "    model_run_suffix = int(latest_model_run_name.split('.')[-1]) + 1\n",
    "    model_run_name = \".\".join([*latest_model_run_name.split('.')[:-1], str(model_run_suffix)])\n",
    "else:\n",
    "    model_run_name = \"0.0.0\"\n",
    "\n",
    "print(f\"Model Name: {model.name} | Model Run Version : {model_run_name}\")\n",
    "model_run = model.create_model_run(model_run_name)\n",
    "model_run.upsert_labels([label.uid for label in val_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cVAWVu1Ikwu4",
   "metadata": {
    "id": "cVAWVu1Ikwu4"
   },
   "outputs": [],
   "source": [
    "pairs = get_label_pairs(val_labels, labels_mea, filter_mismatch = True)\n",
    "for (ground_truth, prediction) in pairs.values():\n",
    "    metrics = []\n",
    "    metrics.extend(feature_miou_metric(ground_truth.annotations, prediction.annotations))\n",
    "    metrics.extend(feature_confusion_matrix_metric(ground_truth.annotations, prediction.annotations))    \n",
    "    prediction.annotations.extend(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-cLS232rFL-R",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-cLS232rFL-R",
    "outputId": "3ce4ebdd-e916-4453-9314-11070b695d2a"
   },
   "outputs": [],
   "source": [
    "upload_task = model_run.add_predictions(f'diagnostics-import-{uuid.uuid4()}', NDJsonConverter.serialize(labels_mea))\n",
    "upload_task.wait_until_done()\n",
    "print(upload_task.state)\n",
    "print(upload_task.errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Lt-f8lxcoIjd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lt-f8lxcoIjd",
    "outputId": "b399a17a-3921-4821-c1b1-dc6c22f478b4"
   },
   "outputs": [],
   "source": [
    "\n",
    "for idx, model_run_data_row in enumerate(model_run.model_run_data_rows()):\n",
    "    if idx == 5:\n",
    "        break\n",
    "    print(model_run_data_row.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UQe1qg5OH0JG",
   "metadata": {
    "id": "UQe1qg5OH0JG"
   },
   "source": [
    "## Dataset Selection\n",
    "* Explore model performance by clicking on the links above\n",
    "* Determine what data the model needs to improve\n",
    "* Use metadata and other features in [Catalog](https://app.labelbox.com/catalog) to select the highest valued data to get labeled.\n",
    "  * If necessary update `mal_dataset_id` update this variable to point to this new dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dreeevOXw8XP",
   "metadata": {
    "id": "dreeevOXw8XP"
   },
   "source": [
    "## Upload for Model Assisted Labeling\n",
    "\n",
    "* Upload via MAL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-NmGLsiYytA2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-NmGLsiYytA2",
    "outputId": "9dfca0f3-be04-43fc-d79b-155afc7f64eb"
   },
   "outputs": [],
   "source": [
    "# Some additional unlabeled data rows\n",
    "dataset = client.get_dataset(mal_dataset_id)\n",
    "\n",
    "\n",
    "# Use ThreadPoolExecutor to parallelize image downloads.\n",
    "# This is still a bit slow due to the amount of processing for each data row.\n",
    "# For larger datasets this has to leverage multiprocessing.\n",
    "\n",
    "labels_mal = LabelList()\n",
    "with ThreadPoolExecutor(4) as executor:\n",
    "    data_rows = dataset.data_rows()\n",
    "    images = [ImageData(url = data_row.row_data, uid = data_row.uid, external_id = data_row.external_id) for data_row in data_rows]\n",
    "    futures = [executor.submit(get_label, image) for image in images]\n",
    "    for future in tqdm(as_completed(futures)):\n",
    "        labels_mal.append(future.result())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1Ix54zUhd9vD",
   "metadata": {
    "id": "1Ix54zUhd9vD"
   },
   "outputs": [],
   "source": [
    "# Create a new project for upload\n",
    "project = client.create_project(name = \"detectron_mal_project\")\n",
    "editor = next(\n",
    "    client.get_labeling_frontends(where=LabelingFrontend.name == 'editor'))\n",
    "project.setup(editor, labels_mal.get_ontology().asdict())\n",
    "project.enable_model_assisted_labeling()\n",
    "project.datasets.connect(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MYI6EXN2i4d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MYI6EXN2i4d6",
    "outputId": "681e041b-aa46-43a4-ad3a-ca553bb95a60"
   },
   "outputs": [],
   "source": [
    "\n",
    "labels_mal.add_url_to_masks(signer) \\\n",
    "      .add_url_to_data(signer) \\\n",
    "      .assign_feature_schema_ids(OntologyBuilder.from_project(project))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yGC_NdL1nNlD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yGC_NdL1nNlD",
    "outputId": "c115979d-4576-4ffa-cef4-a5a8295e7d95"
   },
   "outputs": [],
   "source": [
    "    ndjsons = list(NDJsonConverter.serialize(labels_mal))\n",
    "    upload_task = project.upload_annotations(name=f\"upload-job-{uuid.uuid4()}\",\n",
    "                                         annotations=ndjsons,\n",
    "                                         validate=False)\n",
    "    # Wait for upload to finish\n",
    "    upload_task.wait_until_done()\n",
    "    # Review the upload status\n",
    "    print(upload_task.errors)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "coco_object_sept15.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "researchProj",
   "language": "python",
   "name": "researchproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
